{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main:\n",
    "\n",
    "syms x\n",
    "%COMP1 > COMP2 > 1 (Always)\n",
    "COMP1=5;\n",
    "COMP2=2;\n",
    "COMP3=4;\n",
    "COMP4=5;\n",
    "%Tent map\n",
    "%f1=piecewise(0<=x<=0.5,2*x,0.5<=x<=1,2*(1-x));\n",
    "%f2=piecewise(0<=x<=0.5,1*x,0.5<=x<=1,1*(1-x));\n",
    "%Logistic function\n",
    "f=3.2*x*(1-x);\n",
    "%Period 4\n",
    "%f=piecewise(1<=x<=2,-x+5,2<=x<=3,-2*x+7,3<=x<=4,x-2);\n",
    "%g=x to observe fixed points of the maps.\n",
    "g = x;\n",
    "%for lines y=constant\n",
    "y1=1;\n",
    "y2=2;\n",
    "y3=3;\n",
    "y4=4;\n",
    "y5=5;\n",
    "%period 5 that is not period 3, example from Li yorke\n",
    "%f=piecewise(1<=x<=2,2*x+1,2<=x<=3,7-x,3<=x<=4,10-2*x,4<=x<=5,6-x); \n",
    "[b1,b2]=kcomposition(f,COMP1,COMP2); %b1 stores compositions from COMP1 and b2 stores compositions from COMP2\n",
    "%[k1,k2]=kcomposition(f2,COMP1,COMP2);\n",
    "%[b2,b2prime]=kcomposition(f,COMP4,COMP3);\n",
    "%figure;\n",
    "%fplot(f,[1,3],'r')\n",
    "figure;\n",
    "%fplot(g,[0,1],'k')\n",
    "%hold on;\n",
    "%fplot(b1,[0,1],'b')\n",
    "%hold on;\n",
    "fplot(b1,[0,1],'b') %fplot for symbolic function plot\n",
    "% fplot(y1,[0,6],'r')\n",
    "% hold on;\n",
    "% fplot(y2,[0,6],'r')\n",
    "% hold on;\n",
    "% fplot(y3,[0,6],'r')\n",
    "% hold on;\n",
    "% fplot(y4,[0,6],'r')\n",
    "% hold on;\n",
    "% fplot(y5,[0,6],'r')\n",
    "xlim([0,1])\n",
    "ylim([0,1])\n",
    "grid on\n",
    "\n",
    "Sub routine:\n",
    "\n",
    "function [b0,bprime] = kcomposition(f,k,kprime)\n",
    "%Compose function f with itself k times.\n",
    "%Return any intermediate composition kprime < k and store in bprime.\n",
    "b0=f;\n",
    "for i=1:k-1\n",
    "  if(i==kprime-1)\n",
    "      bprime=compose(f,b0);\n",
    "      b0=bprime;\n",
    "  else\n",
    "      b=compose(f,b0);\n",
    "      b0=b;\n",
    "  end\n",
    "end\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "Python Script and the generated outputs for the Neural Network experiment (To be run in ipython 3 enivronment):\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras import *\n",
    "from sklearn import preprocessing\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils import model_to_dot\n",
    "import keras.backend as K\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "# ### Generate Data\n",
    "# \n",
    "\n",
    "# ### Create the Neural networks for the classification task\n",
    "# \n",
    "# We are going to vary the depth of the neural network as follows (excluding the input and the output layer), with depth d=1,2,3,4,5. We are going to fix the neurons for each layer to be 6, for our classification task. Each model adds one extra hidden layer. We are using the same parameters to train all networks and we require the training error or the classification error to tend to 0 during the training procedure, i.e, we will try and overfit the data (as we try to demonstrate a representation result, rather than a statistical/generalization result). Thus, for the actual training we use the same parameters to train all the different models using \"ADAM\" optimizer and make the epochs to be 200 in order to enable overfitting. To record the training error, we verify that the training saturates by seeing the performance over the epochs and report by default the error in the last epoch.\n",
    "\n",
    "# #### The Classification Task\n",
    "# \n",
    "# We create a classification task that is considered a smoothed version of the n-alternating points problem proposed in Telgarsky (2015), which as we show in our paper is an instance of a period 3 function. So we create 8000 equally spaced points from [0,1] (in increasing order), where the first 1000 points are of label 0, the second 1000 are label 1 and this label alternates every 1000 points. This is what we call a \"smoothed\" alternating point problem. Although, the theory would have used the classical 8-alternating points to argue about the lower bounds, in practice, performing training of deep (4 and above layers) and narrow networks (hidden layers with less than 4 neurons) with very few data points is a major challenge, see for instance [https://arxiv.org/pdf/1808.04947.pdf]. Apart from the separation results that we show in theory, we show empirically that deep networks generally do improve the accuracy in this task compared to the shallow network and in fact a deep network with 5 layers can reach an accuracy of 99.04%. Any addititional uncertainties in the error is generally attributed to the training procedure.\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#Input data goes here-Generate here the classification task\n",
    "n=8000 #2^10 (max depth is 10)\n",
    "x_train=np.zeros((n,1))\n",
    "y_train=np.zeros((n,1))\n",
    "for i in range(n):\n",
    "    x_train[i,0]=(i+1)/n;\n",
    "    if(int(i/1000) % 2 == 0):\n",
    "        y_train[i,0]=0\n",
    "    else:\n",
    "        y_train[i,0]=1\n",
    "\n",
    "\n",
    "# #### Depth d=1\n",
    "\n",
    "# In[97]:\n",
    "\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(6, input_dim=1, activation='relu',\n",
    "                bias_initializer='zeros'))\n",
    "model1.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# In[98]:\n",
    "\n",
    "\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "history=model1.fit(x_train, y_train,\n",
    "          epochs=200,\n",
    "          batch_size=80,verbose=1)\n",
    "\n",
    "\n",
    "# In[99]:\n",
    "\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### Depth d=2\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(6, input_dim=1, activation='relu',\n",
    "                bias_initializer='zeros'))\n",
    "model2.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model2.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "history=model2.fit(x_train, y_train,\n",
    "          epochs=200,\n",
    "          batch_size=80,verbose=1)\n",
    "\n",
    "\n",
    "# In[103]:\n",
    "\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[104]:\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# #### Depth d=3\n",
    "\n",
    "# In[117]:\n",
    "\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(6, input_dim=1, activation='relu',\n",
    "                bias_initializer='zeros'))\n",
    "model3.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model3.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model3.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# In[118]:\n",
    "\n",
    "\n",
    "model3.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "history=model3.fit(x_train, y_train,\n",
    "          epochs=200,\n",
    "          batch_size=80,verbose=1)\n",
    "\n",
    "\n",
    "# In[119]:\n",
    "\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[120]:\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# #### Depth d=4\n",
    "\n",
    "# In[105]:\n",
    "\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(6, input_dim=1, activation='relu',\n",
    "                bias_initializer='zeros'))\n",
    "model4.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model4.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model4.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model4.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# In[106]:\n",
    "\n",
    "\n",
    "model4.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "history=model4.fit(x_train, y_train,\n",
    "          epochs=200,\n",
    "          batch_size=80,verbose=1)\n",
    "\n",
    "\n",
    "# In[107]:\n",
    "\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[108]:\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# #### Depth d=5\n",
    "\n",
    "# In[109]:\n",
    "\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(Dense(6, input_dim=1, activation='relu',\n",
    "                bias_initializer='zeros'))\n",
    "model5.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model5.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model5.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model5.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model5.add(Dense(6, activation='relu',bias_initializer='zeros'))\n",
    "model5.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# In[110]:\n",
    "\n",
    "\n",
    "model5.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "history=model5.fit(x_train, y_train,\n",
    "          epochs=200,\n",
    "          batch_size=80,verbose=1)\n",
    "\n",
    "\n",
    "# In[111]:\n",
    "\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[112]:\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# #### Plot Classifcation Error vs Depth for the above classification task\n",
    "\n",
    "# In[130]:\n",
    "\n",
    "\n",
    "#The values below are obtained from the above training data. The value obtained at the last epoch.\n",
    "err_task=np.array([1-0.5074,1-0.8186,1-0.8643,1-0.8560,1-0.9904])\n",
    "depth=np.array([1,2,3,4,5])\n",
    "plt.plot(depth,err_task,linewidth=3)\n",
    "plt.title('Classification Error vs Depth')\n",
    "plt.ylabel('Classification Error (Fraction of misclassifed points)')\n",
    "plt.xlabel('Depth')\n",
    "plt.xticks(np.arange(1, 6, 1))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
