{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating Function for getting max pages\n",
    "def get_max_pages(user_name):\n",
    "    \"\"\"\n",
    "    :params user_name: username\n",
    "    : return max_page_tag: max numer of pages for user\n",
    "    \"\"\"\n",
    "    \n",
    "    #Setting parameters and getting soup for our user\n",
    "    url = \"https://www.nintendolife.com/users/\" + user_name + \"/games\"\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        html_text = r.text\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    \n",
    "        #try-except block for if a user only has one page\n",
    "        try:\n",
    "            pagination_tags = soup.findAll(\"a\", {\"class\": \"accent accent-bg-hover accent-border\"}) #just pagination tags\n",
    "            page_tag_numbers = []\n",
    "            for tag in pagination_tags:\n",
    "                page_tag_numbers.append(int(tag.contents[0]))\n",
    "            max_page_tag = max(page_tag_numbers)\n",
    "            return max_page_tag\n",
    "        \n",
    "        except:\n",
    "            max_page_tag = 1\n",
    "            return max_page_tag\n",
    "    \n",
    "#Creating Function for getting games and ratings into dataframe\n",
    "def games_ratings_to_df(user_name, max_pages):\n",
    "    \"\"\"\n",
    "    :params user_name: user_name\n",
    "    :params max_pages: max_pages from function get_max_pages()\n",
    "    : return df: scraped pandas dataframe for user_name\n",
    "    : return game_urls: list of urls for games listed on user page\n",
    "    \"\"\"\n",
    "    \n",
    "    #Setting up object holders\n",
    "    cols = [\"username\", \"game\", \"rating\"]\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    games = []\n",
    "    ratings = []\n",
    "    game_urls = []\n",
    "    \n",
    "    #iterating through pages\n",
    "    if max_pages:\n",
    "        for page in range(1, max_pages+1):\n",
    "        #Url is different if on first page rather than other pages\n",
    "            if page == 1:\n",
    "                #Setting parameters and getting first soup for our user\n",
    "                url = \"https://www.nintendolife.com/users/\" + Olliemar28 + \"/games?sort=date&status=own&system=systems%2Fnintendo-switch%2Csystems%2Fswitch-eshop\"\n",
    "                r = requests.get(url)\n",
    "                if r.status_code == 200:\n",
    "                    html_text = r.text\n",
    "                    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "                    \n",
    "                    #Getting our game tags\n",
    "                    spans = soup.findAll(\"span\")\n",
    "                    for i in range(len(spans)):\n",
    "                        #Try-except block for key-errors when filtering for class\n",
    "                        try:\n",
    "                            #conditional for just tag types that are holding game titles\n",
    "                            if spans[i][\"class\"] == [\"title\", \"accent-hover\"]:\n",
    "                                games.append(spans[i].contents[0])\n",
    "                                #Conditional for putting rating or no score - can explain how\n",
    "                                if spans[i+2][\"class\"] == [\"value\"]:\n",
    "                                    ratings.append(spans[i+2].contents[0])\n",
    "                                else:\n",
    "                                    ratings.append(\"No Score\")\n",
    "                        except KeyError:\n",
    "                            pass\n",
    "                \n",
    "                    #Getting game urls\n",
    "                    hrefs = soup.findAll(\"a\", {\"class\": \"title accent-hover\"})\n",
    "                    try:\n",
    "                        for i in range(len(hrefs)):\n",
    "                            url = hrefs[i].attrs[\"href\"]\n",
    "                            game_urls.append(url)\n",
    "                    except:\n",
    "                        pass\n",
    "                    #appending no score for last game (if last game had no score)\n",
    "                    if len(games) > len(ratings):\n",
    "                        ratings.append(\"No Score\")\n",
    "            #Adding the rest of the url pages       \n",
    "            else:\n",
    "                #Setting parameters and getting first soup for our user\n",
    "                url = \"https://www.nintendolife.com/users/\" + Olliemar28 + \"games?sort=date&status=own&system=systems%2Fnintendo-switch%2Csystems%2Fswitch-eshop&page=\" + str(page)\n",
    "                r = requests.get(url)\n",
    "                if r.status_code == 200:\n",
    "                    html_text = r.text\n",
    "                    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "                    \n",
    "                    #Getting our game tags\n",
    "                    spans = soup.findAll(\"span\")\n",
    "                    for i in range(len(spans)):\n",
    "                        #Try-except block for key-errors when filtering for class\n",
    "                        try:\n",
    "                            #conditional for just tag types that are holding game titles\n",
    "                            if spans[i][\"class\"] == [\"title\", \"accent-hover\"]:\n",
    "                                games.append(spans[i].contents[0])\n",
    "                                #Conditional for putting rating or no score - can explain how\n",
    "                                if spans[i+2][\"class\"] == [\"value\"]:\n",
    "                                    ratings.append(spans[i+2].contents[0])\n",
    "                                else:\n",
    "                                    ratings.append(\"No Score\")\n",
    "                        except KeyError:\n",
    "                            pass\n",
    "                        \n",
    "                    #Getting game urls\n",
    "                    hrefs = soup.findAll(\"a\", {\"class\": \"title accent-hover\"})\n",
    "                    for i in range(len(hrefs)):\n",
    "                        url = hrefs[i].attrs[\"href\"]\n",
    "                        game_urls.append(url)\n",
    "                    \n",
    "                    #appending no score for last game (if last game had no score)\n",
    "                    if len(games) > len(ratings):\n",
    "                        ratings.append(\"No Score\")\n",
    "    \n",
    "    #Adding our games and ratings list to dataframe\n",
    "    df[\"game\"] = games\n",
    "    df[\"rating\"] = ratings\n",
    "    df[\"username\"] = user_name\n",
    "    \n",
    "    games_and_urls = list(zip(games, game_urls))\n",
    "    \n",
    "    return df, games_and_urls\n",
    "\n",
    "#Function for getting the max number of pages for a thread\n",
    "def get_max_pages_thread(thread_name):\n",
    "    \"\"\"\n",
    "    :params thread_name: thread_name\n",
    "    : return max_page_tag: max number of pages for thread\n",
    "    \"\"\"\n",
    "    \n",
    "    #Setting parameters and getting soup for our thread\n",
    "    url = \"https://www.nintendolife.com/forums/nintendo-switch/\" + thread_name\n",
    "    r = requests.get(url) \n",
    "    if r.status_code == 200:\n",
    "        html_text = r.text\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    \n",
    "        #try-except block for if a thread only has one page\n",
    "        try:\n",
    "            pagination_tags = soup.findAll(\"a\", {\"class\": \"accent-border accent-bg-hover accent\"}) #just pagination tags\n",
    "            page_tag_numbers = []\n",
    "            for tag in pagination_tags:\n",
    "                page_tag_numbers.append(int(tag.contents[0]))\n",
    "            max_page_tag = max(page_tag_numbers)\n",
    "            return max_page_tag\n",
    "        \n",
    "        except:\n",
    "            max_page_tag = 1\n",
    "            return max_page_tag\n",
    "    \n",
    "#Function for getting all of the users from a thread\n",
    "def get_users(thread_name, max_pages):\n",
    "    '''Function for collecting all users from a thread. \n",
    "    Params: thread name (string), max number of pages ()\n",
    "    : returns deduped_users: De-duplicated list of users'''\n",
    "    \n",
    "    users = []\n",
    "    for i in range(max_pages):\n",
    "        n_posts = i * 20\n",
    "        if n_posts == 0:            \n",
    "            #Setting parameters \n",
    "            url = \"https://www.nintendolife.com/forums/nintendo-switch/\" + thread_name\n",
    "            r = requests.get(url)\n",
    "            if r.status_code == 200:\n",
    "                html_text = r.text\n",
    "                soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "                \n",
    "                #Getting user tags\n",
    "                user_names = soup.findAll(\"a\", {\"class\": \"accent username\"})\n",
    "                for user in user_names:\n",
    "                    users.append(user.contents[0])\n",
    "\n",
    "        else:           \n",
    "            #Setting parameters \n",
    "            url = \"https://www.nintendolife.com/forums/nintendo-switch/\" + thread_name + \"?start=\" + str(n_posts)\n",
    "            r = requests.get(url)\n",
    "            if r.status_code == 200:\n",
    "                html_text = r.text\n",
    "                soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "                \n",
    "                #Getting user tags\n",
    "                user_names = soup.findAll(\"a\", {\"class\": \"accent username\"})\n",
    "                for user in user_names:\n",
    "                    users.append(user.contents[0])\n",
    "                    if len(users) % 1000 == 0:\n",
    "                        print(\"user_count: \", len(users))\n",
    "                \n",
    "    #Deduplicating List\n",
    "    deduped_users = list(set(users))\n",
    "    return deduped_users\n",
    "\n",
    "\n",
    "def get_game_metadata(games_list):\n",
    "    '''Function for getting metadata for games.\n",
    "    :params game_list: list of game names, game urls and game ids\n",
    "    : returns: dataframe with game metadata\n",
    "    '''\n",
    "    #List for appending game metadata\n",
    "    game_metadata = []\n",
    "\n",
    "    for game, url, game_id in games_list:\n",
    "        url = \"http://www.nintendolife.com/\" + url\n",
    "        r = requests.get(url,)\n",
    "        if r.status_code == 200:\n",
    "            html_text = r.text\n",
    "            soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    \n",
    "            #Gets us platform, developer, publisher, number of players\n",
    "            info = soup.findAll(\"dd\", {\"class\": \"first\"})\n",
    "            info_list=[]\n",
    "            for tag in info:\n",
    "                if len(tag.contents) == 1:\n",
    "                    info_list.append(tag.contents[0])\n",
    "            try:\n",
    "                platform = info_list[0]\n",
    "                developer = info_list[1]\n",
    "                publisher = info_list[2]\n",
    "            except:\n",
    "                publisher = \"N/A\"\n",
    "    \n",
    "            #Getting genre\n",
    "            genre_tags = soup.findAll(\"a\")\n",
    "            genre_list = []\n",
    "            for element in genre_tags:\n",
    "                if \"genre\" in element.attrs[\"href\"]:\n",
    "                    genre_list.append(element.contents[0])\n",
    "                    \n",
    "            #Getting Release Date\n",
    "            date_and_price = []\n",
    "            details_tags = soup.findAll(\"li\", {\"class\":\"first\"})\n",
    "            for tag in details_tags:\n",
    "                if len(tag) > 1: \n",
    "                    datestr = str(tag.contents[0])\n",
    "                    if \"<\" not in datestr:\n",
    "                        datestr = datestr.strip(\"(\")\n",
    "                        datestr = datestr.strip(\" \")\n",
    "                        date_and_price.append(datestr)\n",
    "                    pricestr = str(tag.contents[2])\n",
    "                    if \"<\" not in pricestr:\n",
    "                        pricestr = pricestr.strip(\"), \")\n",
    "                        date_and_price.append(pricestr)\n",
    "            \n",
    "            try:           \n",
    "                date = date_and_price[0]\n",
    "                price = date_and_price[1]\n",
    "            except IndexError:\n",
    "                pass\n",
    "                \n",
    "        keys = [\"game\", \"game_id\", \"platform\", \"developer\", \"publisher\", \"genre\", \"release_date\", \"price\"]\n",
    "        values = [game, game_id, platform, developer, publisher, genre_list, date, price]\n",
    "\n",
    "        game_dict = dict(zip(keys, values))\n",
    "        game_metadata.append(game_dict)\n",
    "        \n",
    "        if len(game_metadata) % 100 == 0:\n",
    "            print(\"metadata length: \", len(game_metadata))\n",
    "        \n",
    "    df = pd.DataFrame(game_metadata)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2a851eda2e88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
